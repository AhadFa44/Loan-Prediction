# -*- coding: utf-8 -*-
"""LoanPridction_SVM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16tNfpXIuRLFxOa2YkkJz926clCnNLBKi

Importing libraries and reading dataset from CSV file
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""Removing rows with NULL values from the dataset"""

dataset = pd.read_csv('BankData.csv')
dataset.info()

"""Remove rows with missing values"""

dataset = dataset.dropna()

"""Display unique values for selected columns"""

print(dataset["Gender"].unique())
print(dataset["Married"].unique())
print(dataset["Education"].unique())
print(dataset["Self_Employed"].unique())
print(dataset["Dependents"].unique())
print(dataset["Property_Area"].unique())

"""Normalize selected columns using MinMaxScaler"""

from sklearn.preprocessing import MinMaxScaler
s = MinMaxScaler()
dataset[["ApplicantIncome","CoapplicantIncome","LoanAmount","Loan_Amount_Term","Credit_History"]] = s.fit_transform(dataset[["ApplicantIncome","CoapplicantIncome","LoanAmount","Loan_Amount_Term","Credit_History"]])

"""Encode categorical variables"""

gen = {"Male":1,"Female":0}
m = {"Yes":1,"No":0}
ed = {"Graduate":1,"Not Graduate":0}
s_emp = {"Yes":1,"No":0}
d = {"1":1,"0":0,"2":2,"3+":3}
a = {"Rural":1,"Urban":0,"Semiurban":2}

dataset["Gender"] = [gen[val] for val in dataset["Gender"]]
dataset["Married"] = [m[val] for val in dataset["Married"]]
dataset["Education"] = [ed[val] for val in dataset["Education"]]
dataset["Self_Employed"] = [s_emp[val] for val in dataset["Self_Employed"]]
dataset["Dependents"] = [d[val] for val in dataset["Dependents"]]
dataset["Property_Area"] = [a[val] for val in dataset["Property_Area"]]

"""Split the dataset into training and testing sets"""

y = dataset["Loan_Status"]
X = dataset.drop("Loan_Status",axis=1)
X = pd.get_dummies(X)

print(X)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y)

print(y)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)

print(X_train)

print(X_test)

print(y_train)

print(y_test)

"""Train a Support Vector Machine (SVM) model"""

from sklearn.svm import SVC
svm = SVC()
svm.fit(X_train,y_train)
y_pred = svm.predict(X_test)

"""Evaluate the model"""

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
acc=accuracy_score(y_test, y_pred)
print(acc)
from sklearn.metrics import recall_score
recc=recall_score(y_test, y_pred, average='binary')
print("recall is ",recc)
from sklearn.metrics import precision_score
ps=precision_score(y_test, y_pred, average='binary')
print("precision is ",ps)

"""Accuracy"""

# Code source: Gaël Varoquaux
# Modified for documentation by Jaques Grobler
# License: BSD 3 clause

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from sklearn import datasets
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler

# Define a pipeline to search for the best combination of PCA truncation
# and classifier regularization.
pca = PCA()
# Define a Standard Scaler to normalize inputs
scaler = StandardScaler()

# set the tolerance to a large value to make the example faster
logistic = LogisticRegression(max_iter=10000, tol=0.1)
pipe = Pipeline(steps=[("scaler", scaler), ("pca", pca), ("logistic", logistic)])

X_train, y_train = datasets.load_digits(return_X_y=True)
# Parameters of pipelines can be set using ‘__’ separated parameter names:
param_grid = {
    "pca__n_components": [3,4,5,6,7],
    "logistic__C": np.logspace(-4, 4, 4),
}
search = GridSearchCV(pipe, param_grid, n_jobs=2)
search.fit(X_train, y_train)
print("Best parameter (CV score=%0.3f):" % search.best_score_)
print(search.best_params_)

# Plot the PCA spectrum
pca.fit(X_train)
print("heeeeeeereeeee",pca)
fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))
ax0.plot(
    np.arange(1, pca.n_components_ + 1), pca.explained_variance_ratio_, "+", linewidth=2
)
ax0.set_ylabel("PCA explained variance ratio")

ax0.axvline(
    search.best_estimator_.named_steps["pca"].n_components,
    linestyle=":",
    label="n_components chosen",
)
ax0.legend(prop=dict(size=12))

# For each number of components, find the best classifier results
results = pd.DataFrame(search.cv_results_)
components_col = "param_pca__n_components"
best_clfs = results.groupby(components_col).apply(
    lambda g: g.nlargest(1, "mean_test_score")
)

best_clfs.plot(
    x=components_col, y="mean_test_score", yerr="std_test_score", legend=False, ax=ax1
)
ax1.set_ylabel("Classification accuracy (val)")
ax1.set_xlabel("n_components")

plt.xlim(-1, 10)

plt.tight_layout()
plt.show()
print(y_test)

print(cm)
from sklearn.metrics import recall_score
recc=recall_score(y_test, y_pred, average='binary')
print("recall is ",recc)
from sklearn.metrics import precision_score
ps=precision_score(y_test, y_pred, average='binary')
print("precision is ",ps)

"""Apply K-fold cross-validation"""

#Applying K-fold cross validation
from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = svm, X = X_train, y = y_train, cv = 10)

print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))
from sklearn.metrics import recall_score
recc=recall_score(y_test, y_pred, average='binary')
print("recall is ",recc)
from sklearn.metrics import precision_score
ps=precision_score(y_test, y_pred, average='binary')
print("precision is ",ps)

print("-----------------------------------------------------")
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

for col in dataset.columns:
  print(col)

"""Make a loan prediction based on user input"""

# Get user input
gender = input("Enter gender (Male/Female): ")
married = input("Enter if married (Yes/No): ")
educ = input("Enter Education (Graduate/Not Graduate): ")
emp = input("Self_Employed (Yes/No): ")
dept = input("Dependents (0/1/2/3+): ")
app = float(input("ApplicantIncome: "))
co = float(input("CoapplicantIncome: "))
loan = float(input("LoanAmount: "))
term = float(input("Loan_Amount_Term: "))
credit = float(input("Credit_History (0 or 1): "))
prop = input("Property_Area (Urban/Semiurban/Rural): ")

import numpy as np
from sklearn.preprocessing import MinMaxScaler

# Ensure the scaler 's' is fitted on the training data if it was used for X_train
# Assuming 's' is the scaler already fitted on the training data in a previous step.
# If 's' was not fitted, you would need to fit it here or earlier.

# Correctly encode categorical features using the defined dictionaries
gender_encoded = gen[data_to_predict[0]]
married_encoded = m[data_to_predict[1]]
educ_encoded = ed[data_to_predict[2]]
emp_encoded = s_emp[data_to_predict[3]]
dept_encoded = d[data_to_predict[4]]
prop_encoded = a[data_to_predict[10]]

# Numerical features
applicant_income = data_to_predict[5]
coapplicant_income = data_to_predict[6]
loan_amount = data_to_predict[7]
loan_amount_term = data_to_predict[8]
credit_history = data_to_predict[9]

# Collect numerical features in an array and scale them
numerical_features = np.array([[applicant_income, coapplicant_income, loan_amount, loan_amount_term, credit_history]])

# Assuming 's' is the MinMaxScaler fitted on the training data
# If 's' was not fitted on the training data, you need to fit it here or earlier.
# Example if s was not fitted:
# numerical_data_for_fitting = dataset[["ApplicantIncome","CoapplicantIncome","LoanAmount","Loan_Amount_Term","Credit_History"]]
# s = MinMaxScaler()
# s.fit(numerical_data_for_fitting)
scaled_numerical_features = s.transform(numerical_features)


# Combine all features into a single array for prediction
# The order of features must match the order of features in X_train
# Based on the output of BxqXALGWBHgd and X_train structure, the order is likely:
# Gender, Married, Education, Self_Employed, Dependents, ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount_Term, Credit_History, Property_Area
encoded_data = np.array([[gender_encoded, married_encoded, educ_encoded, emp_encoded, dept_encoded,
                        scaled_numerical_features[0, 0], scaled_numerical_features[0, 1],
                        scaled_numerical_features[0, 2], scaled_numerical_features[0, 3],
                        scaled_numerical_features[0, 4], prop_encoded]])

# ✅ Predict
pred = svm.predict(encoded_data)
print("Prediction:", pred)

# Display prediction result
if pred[0] == 1:
    print("Yes, you can get a loan.")
else:
    print("Sorry, you cannot get a loan.")

"""PRdiction output"""